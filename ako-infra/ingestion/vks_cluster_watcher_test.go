/*
 * Copyright Â© 2025 Broadcom Inc. and/or its subsidiaries. All Rights Reserved.
 * All Rights Reserved.
* Licensed under the Apache License, Version 2.0 (the "License");
* you may not use this file except in compliance with the License.
* You may obtain a copy of the License at
*   http://www.apache.org/licenses/LICENSE-2.0
* Unless required by applicable law or agreed to in writing, software
* distributed under the License is distributed on an "AS IS" BASIS,
* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
* See the License for the specific language governing permissions and
* limitations under the License.
*/

// @AI-Generated
// This test file was generated by Claude AI assistant.

package ingestion

import (
	"context"
	"fmt"
	"os"
	"sync"
	"testing"
	"time"

	"github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/ako-infra/webhook"
	"github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/internal/k8s"
	"github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/internal/lib"
	akoapisv1beta1 "github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/pkg/apis/ako/v1beta1"
	"github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/pkg/utils"

	v1beta1crdfake "github.com/vmware/load-balancer-and-ingress-services-for-kubernetes/pkg/client/v1beta1/clientset/versioned/fake"

	corev1 "k8s.io/api/core/v1"
	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
	"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured"
	"k8s.io/apimachinery/pkg/runtime"
	"k8s.io/apimachinery/pkg/runtime/schema"
	"k8s.io/client-go/dynamic/fake"
	k8sfake "k8s.io/client-go/kubernetes/fake"
	"k8s.io/client-go/tools/cache"
)

// Global sync.Once to ensure informers are started only once across all tests
var (
	informerStartOnce sync.Once
	stopChInformers   chan struct{}
)

// startInformersOnce starts informers only once using sync.Once
func startInformersOnce() {
	informerStartOnce.Do(func() {
		// Initialize the stop channel for informers
		stopChInformers = make(chan struct{})

		// Start the secret informer
		go func() {
			utils.GetInformers().SecretInformer.Informer().Run(stopChInformers)
		}()

		// Wait for secret informer cache to sync
		if !cache.WaitForCacheSync(stopChInformers, utils.GetInformers().SecretInformer.Informer().HasSynced) {
			// Log warning but don't fail - this is expected in test environment
			fmt.Printf("Warning: secret informer cache sync timed out (expected in test environment)\n")
		}
	})
}

// createTypedAviInfraSetting creates a typed AviInfraSetting for the fake CRD client
func createTypedAviInfraSetting(name string) *akoapisv1beta1.AviInfraSetting {
	t1lr := "/orgs/test-org/projects/test-project/vpcs/test-vpc"
	return &akoapisv1beta1.AviInfraSetting{
		TypeMeta: metav1.TypeMeta{
			APIVersion: "ako.vmware.com/v1beta1",
			Kind:       "AviInfraSetting",
		},
		ObjectMeta: metav1.ObjectMeta{
			Name: name,
		},
		Spec: akoapisv1beta1.AviInfraSettingSpec{
			SeGroup: akoapisv1beta1.AviInfraSettingSeGroup{
				Name: "test-se-group",
			},
			NSXSettings: akoapisv1beta1.AviInfraNSXSettings{
				T1LR: &t1lr,
			},
		},
	}
}

// VKSTestSetup contains all the clients and resources needed for VKS testing
type VKSTestSetup struct {
	KubeClient       *k8sfake.Clientset
	DynamicClient    *fake.FakeDynamicClient
	Namespace        *corev1.Namespace
	ClusterBootstrap *unstructured.Unstructured
	AviInfraSetting  *unstructured.Unstructured
	StopCh           chan struct{}
}

// setupVKSTest creates a complete VKS test environment with all required resources and informers
func setupVKSTest(t *testing.T, clusterName, namespaceName, cniRefName string) *VKSTestSetup {
	// Set VCF_CLUSTER environment variable to enable dynamic informers
	oldVCFCluster := os.Getenv("VCF_CLUSTER")
	os.Setenv("VCF_CLUSTER", "true")
	t.Cleanup(func() {
		if oldVCFCluster == "" {
			os.Unsetenv("VCF_CLUSTER")
		} else {
			os.Setenv("VCF_CLUSTER", oldVCFCluster)
		}
	})
	// Create namespace with required VKS annotations
	namespace := &corev1.Namespace{
		ObjectMeta: metav1.ObjectMeta{
			Name: namespaceName,
			Annotations: map[string]string{
				lib.WCPSEGroup:                 "test-se-group",
				lib.TenantAnnotation:           "test-tenant",
				lib.InfraSettingNameAnnotation: "test-aviinfrasetting",
			},
		},
	}

	// Create ClusterBootstrap for CNI detection
	clusterBootstrap := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "run.tanzu.vmware.com/v1alpha3",
			"kind":       "ClusterBootstrap",
			"metadata": map[string]interface{}{
				"name":      clusterName,
				"namespace": namespaceName,
			},
			"spec": map[string]interface{}{
				"cni": map[string]interface{}{
					"refName": cniRefName,
				},
			},
		},
	}

	// Create AviInfraSetting for T1LR configuration
	aviInfraSetting := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "ako.vmware.com/v1beta1",
			"kind":       "AviInfraSetting",
			"metadata": map[string]interface{}{
				"name": "test-aviinfrasetting",
			},
			"spec": map[string]interface{}{
				"seGroup": map[string]interface{}{
					"name": "test-se-group",
				},
				"nsxSettings": map[string]interface{}{
					"t1lr": "/orgs/test-org/projects/test-project/vpcs/test-vpc",
				},
			},
		},
	}

	// Create Kubernetes and dynamic clients
	kubeClient := k8sfake.NewSimpleClientset(namespace)

	// Set up dynamic client with custom list kinds to support List operations
	gvrToKind := map[schema.GroupVersionResource]string{
		lib.ClusterGVR: "clustersList",
		{
			Group:    "ako.vmware.com",
			Version:  "v1beta1",
			Resource: "aviinfrasettings",
		}: "aviinfrasettingsList",
		{
			Group:    "run.tanzu.vmware.com",
			Version:  "v1alpha3",
			Resource: "clusterbootstraps",
		}: "clusterbootstrapsList",
	}
	dynamicClient := fake.NewSimpleDynamicClientWithCustomListKinds(runtime.NewScheme(), gvrToKind, clusterBootstrap, aviInfraSetting)

	// Initialize regular informers (including namespace informer)
	registeredInformers := []string{
		utils.SecretInformer,
		utils.NSInformer,
	}
	utils.NewInformers(utils.KubeClientIntf{ClientSet: kubeClient}, registeredInformers, nil)

	// Initialize dynamic informers for cluster operations
	lib.SetDynamicClientSet(dynamicClient)
	lib.NewDynamicInformers(dynamicClient, true)

	// Set up CRD client for AviInfraSetting access
	akoControlConfig := lib.AKOControlConfig()
	if akoControlConfig == nil {
		t.Fatalf("AKOControlConfig is nil")
	}

	// Create and set up the CRD client with a test AviInfraSetting
	testAviInfraSetting := createTypedAviInfraSetting("test-aviinfrasetting")
	v1beta1crdClient := v1beta1crdfake.NewSimpleClientset(testAviInfraSetting)
	akoControlConfig.SetCRDClientsetAndEnableInfraSettingParam(v1beta1crdClient)

	// Initialize CRD informers
	k8s.NewCRDInformers()

	// Start the AviInfraSetting informer and wait for cache sync
	stopCh := make(chan struct{})
	go func() {
		defer close(stopCh)
		time.Sleep(30 * time.Second) // Allow test to complete
	}()
	go lib.RunAviInfraSettingInformer(stopCh)

	// Give the informer time to sync
	time.Sleep(100 * time.Millisecond)

	// Start informers once using sync.Once to avoid multiple starts
	startInformersOnce()

	// Add namespace to informer cache for tests that need it
	err := utils.GetInformers().NSInformer.Informer().GetStore().Add(namespace)
	if err != nil {
		t.Logf("Warning: Failed to add namespace to informer cache: %v", err)
	}

	return &VKSTestSetup{
		KubeClient:       kubeClient,
		DynamicClient:    dynamicClient,
		Namespace:        namespace,
		ClusterBootstrap: clusterBootstrap,
		AviInfraSetting:  aviInfraSetting,
		StopCh:           stopCh,
	}
}

// Cleanup cleans up test resources
func (setup *VKSTestSetup) Cleanup() {
	if setup.StopCh != nil {
		close(setup.StopCh)
	}
}

func TestVKSClusterWatcher_GetClusterPhase(t *testing.T) {
	tests := []struct {
		name          string
		cluster       *unstructured.Unstructured
		expectedPhase string
	}{
		{
			name: "Provisioning cluster",
			cluster: &unstructured.Unstructured{
				Object: map[string]interface{}{
					"status": map[string]interface{}{
						"phase": "Provisioning",
					},
				},
			},
			expectedPhase: "Provisioning",
		},
		{
			name: "Provisioned cluster",
			cluster: &unstructured.Unstructured{
				Object: map[string]interface{}{
					"status": map[string]interface{}{
						"phase": "Provisioned",
					},
				},
			},
			expectedPhase: "Provisioned",
		},
		{
			name: "Deleting cluster",
			cluster: &unstructured.Unstructured{
				Object: map[string]interface{}{
					"status": map[string]interface{}{
						"phase": "Deleting",
					},
				},
			},
			expectedPhase: "Deleting",
		},
		{
			name: "No status",
			cluster: &unstructured.Unstructured{
				Object: map[string]interface{}{},
			},
			expectedPhase: "",
		},
		{
			name: "No phase",
			cluster: &unstructured.Unstructured{
				Object: map[string]interface{}{
					"status": map[string]interface{}{},
				},
			},
			expectedPhase: "",
		},
	}

	setup := setupVKSTest(t, "test-cluster", "test-namespace", "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
	defer setup.Cleanup()
	watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			phase := watcher.GetClusterPhase(tt.cluster)
			if phase != tt.expectedPhase {
				t.Errorf("Expected phase %s, got %s", tt.expectedPhase, phase)
			}
		})
	}
}

func TestVKSClusterWatcher_ClusterDeletionAndCleanup(t *testing.T) {
	// Comprehensive test for cluster deletion and secret cleanup scenarios
	tests := []struct {
		name             string
		clusterName      string
		clusterNamespace string
		secretExists     bool
		expectError      bool
	}{
		{
			name:             "Delete existing secret",
			clusterName:      "test-cluster",
			clusterNamespace: "test-namespace",
			secretExists:     true,
			expectError:      false,
		},
		{
			name:             "Delete non-existing secret",
			clusterName:      "missing-cluster",
			clusterNamespace: "test-namespace",
			secretExists:     false,
			expectError:      false, // Should not error on missing secret
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			setup := setupVKSTest(t, tt.clusterName, tt.clusterNamespace, "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
			defer setup.Cleanup()

			// Create secret if it should exist
			if tt.secretExists {
				secretName := fmt.Sprintf("%s-avi-secret", tt.clusterName)
				secret := &corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      secretName,
						Namespace: tt.clusterNamespace,
					},
				}
				_, err := setup.KubeClient.CoreV1().Secrets(tt.clusterNamespace).Create(context.Background(), secret, metav1.CreateOptions{})
				if err != nil {
					t.Fatalf("Failed to create test secret: %v", err)
				}
			}

			watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

			ctx := context.Background()
			err := watcher.cleanupClusterDependencies(ctx, tt.clusterName, tt.clusterNamespace, "")

			if tt.expectError && err == nil {
				t.Error("Expected error but got none")
			}

			if !tt.expectError && err != nil {
				t.Errorf("Expected no error but got: %v", err)
			}

			// Verify secret was deleted if it existed
			if tt.secretExists {
				secretName := fmt.Sprintf("%s-avi-secret", tt.clusterName)
				_, err := setup.KubeClient.CoreV1().Secrets(tt.clusterNamespace).Get(ctx, secretName, metav1.GetOptions{})
				if err == nil {
					t.Error("Expected secret to be deleted but it still exists")
				}
			}
		})
	}
}

func TestVKSClusterWatcher_HandleProvisionedCluster(t *testing.T) {
	// This test focuses on VKS management decision logic with proper mocks
	// Tests the key scenarios for cluster provisioning and VKS label handling

	// Set up mock controller IP
	lib.SetControllerIP("10.10.10.10")

	tests := []struct {
		name             string
		clusterName      string
		clusterNamespace string
		vksLabel         string
		secretExists     bool
		expectedAction   string // "create", "delete", "none"
		expectError      bool
	}{
		{
			name:             "Should manage cluster with no secret - create secret",
			clusterName:      "test-cluster-1",
			clusterNamespace: "test-namespace-1",
			vksLabel:         webhook.VKSManagedLabelValueTrue,
			secretExists:     false,
			expectedAction:   "create",
			expectError:      false,
		},
		{
			name:             "Should not manage cluster with secret - delete secret",
			clusterName:      "test-cluster-2",
			clusterNamespace: "test-namespace-2",
			vksLabel:         webhook.VKSManagedLabelValueFalse,
			secretExists:     true,
			expectedAction:   "delete",
			expectError:      false,
		},
		{
			name:             "Should manage cluster with secret - update if needed",
			clusterName:      "test-cluster-3",
			clusterNamespace: "test-namespace-3",
			vksLabel:         webhook.VKSManagedLabelValueTrue,
			secretExists:     true,
			expectedAction:   "none", // May update but secret should still exist
			expectError:      false,
		},
		{
			name:             "Should not manage cluster with no secret - no action",
			clusterName:      "test-cluster-4",
			clusterNamespace: "test-namespace-4",
			vksLabel:         webhook.VKSManagedLabelValueFalse,
			secretExists:     false,
			expectedAction:   "none",
			expectError:      false,
		},
		{
			name:             "No VKS label with secret - delete secret",
			clusterName:      "test-cluster-5",
			clusterNamespace: "test-namespace-5",
			vksLabel:         "",
			secretExists:     true,
			expectedAction:   "delete",
			expectError:      false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Set up complete VKS test environment
			setup := setupVKSTest(t, tt.clusterName, tt.clusterNamespace, "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
			defer setup.Cleanup()

			// Create secret if it should exist for this test
			secretName := fmt.Sprintf("%s-avi-secret", tt.clusterName)
			if tt.secretExists {
				secret := &corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      secretName,
						Namespace: tt.clusterNamespace,
					},
				}
				_, err := setup.KubeClient.CoreV1().Secrets(tt.clusterNamespace).Create(context.Background(), secret, metav1.CreateOptions{})
				if err != nil {
					t.Fatalf("Failed to create test secret: %v", err)
				}

				// Manually add secret to informer cache to ensure proper sync
				err = utils.GetInformers().SecretInformer.Informer().GetStore().Add(secret)
				if err != nil {
					t.Logf("Warning: Failed to add secret to informer cache: %v", err)
				}
			}

			// Start informers and ensure proper cache sync
			stopCh := make(chan struct{})
			defer close(stopCh)

			// Run informer in background
			go func() {
				utils.GetInformers().SecretInformer.Informer().Run(stopCh)
			}()

			// Wait for cache sync with proper timeout
			if !cache.WaitForCacheSync(stopCh, utils.GetInformers().SecretInformer.Informer().HasSynced) {
				t.Log("Warning: Cache sync failed, but continuing with test")
			}

			// Give more time for cache to be fully ready and populated
			time.Sleep(200 * time.Millisecond)

			// Manually add existing secret to informer cache to ensure proper sync
			if tt.secretExists {
				// Get the secret that was created in the fake client
				existingSecret, err := setup.KubeClient.CoreV1().Secrets(tt.clusterNamespace).Get(context.Background(), secretName, metav1.GetOptions{})
				if err == nil {
					// Add it to the informer cache directly
					err = utils.GetInformers().SecretInformer.Informer().GetStore().Add(existingSecret)
					if err != nil {
						t.Logf("Warning: Failed to add secret to informer cache: %v", err)
					} else {
						t.Logf("Successfully added secret %s to informer cache", secretName)
					}
				} else {
					t.Logf("Warning: Failed to get existing secret from kube client: %v", err)
				}

				// Give time for cache update
				time.Sleep(50 * time.Millisecond)
			}

			// Create mock ClusterBootstrap for CNI detection (needed for any secret creation scenarios)
			clusterBootstrap := &unstructured.Unstructured{
				Object: map[string]interface{}{
					"apiVersion": "run.tanzu.vmware.com/v1alpha3",
					"kind":       "ClusterBootstrap",
					"metadata": map[string]interface{}{
						"name":      tt.clusterName,
						"namespace": tt.clusterNamespace,
					},
					"spec": map[string]interface{}{
						"cni": map[string]interface{}{
							"refName": "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1",
						},
					},
				},
			}

			// Create AviInfraSetting for T1LR configuration (required for namespace config)
			aviInfraSetting := &unstructured.Unstructured{
				Object: map[string]interface{}{
					"apiVersion": "ako.vmware.com/v1beta1",
					"kind":       "AviInfraSetting",
					"metadata": map[string]interface{}{
						"name": "test-aviinfrasetting",
					},
					"spec": map[string]interface{}{
						"seGroup": map[string]interface{}{
							"name": "test-se-group",
						},
						"nsxSettings": map[string]interface{}{
							"t1lr": "/orgs/test-org/projects/test-project/vpcs/test-vpc",
						},
					},
				},
			}

			dynamicClient := fake.NewSimpleDynamicClient(runtime.NewScheme(), clusterBootstrap, aviInfraSetting)

			// Initialize CRD informers for AviInfraSetting (required for namespace config)
			lib.SetDynamicClientSet(dynamicClient)
			lib.NewDynamicInformers(dynamicClient, true)

			// Set up CRD client for AviInfraSetting access (from integration test pattern)
			akoControlConfig := lib.AKOControlConfig()
			if akoControlConfig == nil {
				t.Fatalf("AKOControlConfig is nil")
			}

			// Create and set up the CRD client with a test AviInfraSetting
			testAviInfraSetting := createTypedAviInfraSetting("test-aviinfrasetting")
			v1beta1crdClient := v1beta1crdfake.NewSimpleClientset(testAviInfraSetting)
			akoControlConfig.SetCRDClientsetAndEnableInfraSettingParam(v1beta1crdClient)

			// Initialize CRD informers (this step is critical for getNamespaceConfig to work)
			k8s.NewCRDInformers()

			// Start the AviInfraSetting informer and wait for cache sync (like integration tests)
			aviStopCh := make(chan struct{})
			defer close(aviStopCh)
			go func() {
				defer close(aviStopCh)
				time.Sleep(30 * time.Second) // Allow test to complete
			}()
			go lib.RunAviInfraSettingInformer(aviStopCh)

			// Give the informer time to sync
			time.Sleep(100 * time.Millisecond)

			watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

			// Set up mock credentials function for tests that need secret creation
			if tt.expectedAction == "create" || (tt.expectedAction == "none" && tt.vksLabel == webhook.VKSManagedLabelValueTrue) {
				watcher.SetTestMode(func(clusterNameWithUID, operationalTenant string) (*lib.ClusterCredentials, error) {
					return &lib.ClusterCredentials{
						Username: fmt.Sprintf("vks-cluster-%s-user", clusterNameWithUID),
						Password: "mock-password",
					}, nil
				})
			}

			// Create cluster with appropriate label
			cluster := &unstructured.Unstructured{
				Object: map[string]interface{}{
					"metadata": map[string]interface{}{
						"name":      tt.clusterName,
						"namespace": tt.clusterNamespace,
						"uid":       "test-uid-123",
					},
				},
			}

			if tt.vksLabel != "" {
				cluster.SetLabels(map[string]string{
					webhook.VKSManagedLabel: tt.vksLabel,
				})
			}

			err := watcher.HandleProvisionedCluster(cluster)

			if tt.expectError && err == nil {
				t.Error("Expected error but got none")
			}

			if !tt.expectError && err != nil {
				t.Errorf("Expected no error but got: %v", err)
			}

			// Verify expected action was taken
			ctx := context.Background()
			finalSecret, secretErr := setup.KubeClient.CoreV1().Secrets(tt.clusterNamespace).Get(ctx, secretName, metav1.GetOptions{})
			secretExistsAfter := secretErr == nil

			switch tt.expectedAction {
			case "create":
				if !secretExistsAfter {
					t.Error("Expected secret to be created but it doesn't exist")
				} else {
					expectedFields := []string{"username", "controllerIP", "clusterName"}
					for _, field := range expectedFields {
						if _, exists := finalSecret.Data[field]; !exists {
							t.Errorf("Expected field %s not found in created secret", field)
						}
					}
				}
			case "delete":
				if secretExistsAfter {
					t.Error("Expected secret to be deleted but it still exists")
				}
			case "none":
				// For "none" action, secret existence should match initial state
				// BUT for VKS managed clusters, the secret might be created/updated even if it existed
				if tt.vksLabel == webhook.VKSManagedLabelValueTrue {
					// VKS managed cluster should have a secret
					if !secretExistsAfter {
						t.Error("VKS managed cluster should have a secret")
					}
				} else {
					// Non-VKS managed cluster should match initial state
					if secretExistsAfter != tt.secretExists {
						t.Errorf("Expected secret existence to remain %v but it's %v", tt.secretExists, secretExistsAfter)
					}
				}
			}

			t.Logf("Test %s completed: secretExists=%v, secretExistsAfter=%v, action=%s",
				tt.name, tt.secretExists, secretExistsAfter, tt.expectedAction)
		})
	}
}

func TestVKSClusterWatcher_ProcessClusterEvent(t *testing.T) {
	// Set up mock controller IP
	lib.SetControllerIP("10.10.10.10")

	tests := []struct {
		name          string
		key           string
		clusterExists bool
		clusterPhase  string
		vksLabel      string
		expectError   bool
	}{
		{
			name:          "Process provisioning cluster",
			key:           "test-namespace/test-cluster",
			clusterExists: true,
			clusterPhase:  ClusterPhaseProvisioning,
			vksLabel:      webhook.VKSManagedLabelValueTrue,
			expectError:   false,
		},
		{
			name:          "Process provisioned cluster",
			key:           "test-namespace/test-cluster",
			clusterExists: true,
			clusterPhase:  ClusterPhaseProvisioned,
			vksLabel:      webhook.VKSManagedLabelValueTrue,
			expectError:   false,
		},
		{
			name:          "Process deleting cluster",
			key:           "test-namespace/test-cluster",
			clusterExists: true,
			clusterPhase:  ClusterPhaseDeleting,
			vksLabel:      webhook.VKSManagedLabelValueTrue,
			expectError:   false,
		},
		{
			name:          "Process non-existent cluster",
			key:           "test-namespace/missing-cluster",
			clusterExists: false,
			clusterPhase:  "",
			vksLabel:      "",
			expectError:   false, // Should handle deletion case
		},
		{
			name:          "Invalid key format",
			key:           "invalid-key",
			clusterExists: false,
			clusterPhase:  "",
			vksLabel:      "",
			expectError:   false, // Should handle gracefully
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Set up complete VKS test environment
			setup := setupVKSTest(t, "test-cluster", "test-namespace", "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
			defer setup.Cleanup()

			// T1LR environment already set up by setupVKSTest

			// Create cluster if it should exist
			if tt.clusterExists {
				cluster := &unstructured.Unstructured{
					Object: map[string]interface{}{
						"apiVersion": "cluster.x-k8s.io/v1beta2",
						"kind":       "Cluster",
						"metadata": map[string]interface{}{
							"name":      "test-cluster",
							"namespace": "test-namespace",
							"uid":       "test-uid-123",
						},
						"status": map[string]interface{}{
							"phase": tt.clusterPhase,
						},
					},
				}

				if tt.vksLabel != "" {
					cluster.SetLabels(map[string]string{
						webhook.VKSManagedLabel: tt.vksLabel,
					})
				}

				// Add cluster to fake client and dynamic informer cache
				setup.DynamicClient.Tracker().Add(cluster)

				// Also add to dynamic informer cache if available
				if lib.GetDynamicInformers() != nil && lib.GetDynamicInformers().ClusterInformer != nil {
					err := lib.GetDynamicInformers().ClusterInformer.Informer().GetStore().Add(cluster)
					if err != nil {
						t.Logf("Warning: Failed to add cluster to dynamic informer cache: %v", err)
					}
				}

				// Add ClusterBootstrap if this is a provisioning/provisioned cluster test
				if (tt.clusterPhase == ClusterPhaseProvisioning || tt.clusterPhase == ClusterPhaseProvisioned) && tt.vksLabel == webhook.VKSManagedLabelValueTrue {
					clusterBootstrap := &unstructured.Unstructured{
						Object: map[string]interface{}{
							"apiVersion": "run.tanzu.vmware.com/v1alpha3",
							"kind":       "ClusterBootstrap",
							"metadata": map[string]interface{}{
								"name":      "test-cluster",
								"namespace": "test-namespace",
							},
							"spec": map[string]interface{}{
								"cni": map[string]interface{}{
									"refName": "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1",
								},
							},
						},
					}
					setup.DynamicClient.Tracker().Add(clusterBootstrap)
				}
			}

			watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

			// Set up mock credentials function for provisioning/provisioned cluster tests
			if (tt.clusterPhase == ClusterPhaseProvisioning || tt.clusterPhase == ClusterPhaseProvisioned) && tt.vksLabel == webhook.VKSManagedLabelValueTrue {
				watcher.SetTestMode(func(clusterNameWithUID, operationalTenant string) (*lib.ClusterCredentials, error) {
					return &lib.ClusterCredentials{
						Username: fmt.Sprintf("vks-cluster-%s-user", clusterNameWithUID),
						Password: "mock-password",
					}, nil
				})
			}

			err := watcher.ProcessClusterEvent(tt.key)

			if tt.expectError && err == nil {
				t.Error("Expected error but got none")
			}

			if !tt.expectError && err != nil {
				t.Errorf("Expected no error but got: %v", err)
			}
		})
	}
}

func TestVKSClusterWatcher_WorkerIntegration(t *testing.T) {
	// Test the worker integration with proper mocking

	// Set up mock controller IP
	lib.SetControllerIP("10.10.10.10")

	setup := setupVKSTest(t, "test-cluster", "test-namespace", "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
	defer setup.Cleanup()

	watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

	// Set up mock credentials function
	watcher.SetTestMode(func(clusterName, operationalTenant string) (*lib.ClusterCredentials, error) {
		return &lib.ClusterCredentials{
			Username: fmt.Sprintf("vks-cluster-%s-user", clusterName),
			Password: "mock-password",
		}, nil
	})

	// Create a test cluster
	cluster := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "cluster.x-k8s.io/v1beta2",
			"kind":       "Cluster",
			"metadata": map[string]interface{}{
				"name":      "test-cluster",
				"namespace": "test-namespace",
				"uid":       "test-uid-123",
				"labels": map[string]interface{}{
					webhook.VKSManagedLabel: webhook.VKSManagedLabelValueTrue,
				},
			},
			"status": map[string]interface{}{
				"phase": ClusterPhaseProvisioned,
			},
		},
	}

	// Add cluster to fake client and dynamic informer cache
	setup.DynamicClient.Tracker().Add(cluster)

	// Also add to dynamic informer cache if available
	if lib.GetDynamicInformers() != nil && lib.GetDynamicInformers().ClusterInformer != nil {
		err := lib.GetDynamicInformers().ClusterInformer.Informer().GetStore().Add(cluster)
		if err != nil {
			t.Logf("Warning: Failed to add cluster to dynamic informer cache: %v", err)
		}
	}

	// Start the worker
	err := watcher.Start(make(<-chan struct{}))
	if err != nil {
		t.Fatalf("Failed to start watcher: %v", err)
	}

	// Enqueue a cluster event
	watcher.EnqueueCluster(cluster, "ADD")

	// Process one work item
	processed := watcher.ProcessNextWorkItem()
	if !processed {
		t.Error("Expected work item to be processed")
	}

	// Verify that the secret was created successfully (with retry)
	secretName := "test-cluster-avi-secret"
	var secret *corev1.Secret
	ctx := context.Background()
	for i := 0; i < 10; i++ {
		secret, err = setup.KubeClient.CoreV1().Secrets("test-namespace").Get(ctx, secretName, metav1.GetOptions{})
		if err == nil {
			break
		}
		time.Sleep(50 * time.Millisecond)
	}
	if err != nil {
		t.Fatalf("Expected secret to be created, got error: %v", err)
	}

	if secret.Name != secretName {
		t.Errorf("Expected secret name %s, got %s", secretName, secret.Name)
	}

	// Verify mock credentials are in the secret
	expectedUsername := "vks-cluster-test-namespace-test-cluster-test-uid-user"
	if string(secret.Data["username"]) != expectedUsername {
		t.Errorf("Expected username to be '%s', got '%s'", expectedUsername, string(secret.Data["username"]))
	}

	// Stop the watcher
	watcher.Stop()
}

func TestVKSClusterWatcher_SecretIdempotency(t *testing.T) {
	// Test that handling the same cluster multiple times is idempotent

	// Set up mock controller IP
	lib.SetControllerIP("10.10.10.10")

	// Set up environment for T1LR fallback
	os.Setenv("NSXT_T1_LR", "/orgs/test-org/projects/test-project/vpcs/test-vpc")
	defer os.Unsetenv("NSXT_T1_LR")

	setup := setupVKSTest(t, "test-cluster-idempotency", "test-namespace-idempotency", "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
	defer setup.Cleanup()

	watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

	// Set up mock credentials function
	watcher.SetTestMode(func(clusterName, operationalTenant string) (*lib.ClusterCredentials, error) {
		return &lib.ClusterCredentials{
			Username: fmt.Sprintf("vks-cluster-%s-user", clusterName),
			Password: "mock-password",
		}, nil
	})

	cluster := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"metadata": map[string]interface{}{
				"name":      "test-cluster-idempotency",
				"namespace": "test-namespace-idempotency",
				"uid":       "test-uid-123",
				"labels": map[string]interface{}{
					webhook.VKSManagedLabel: webhook.VKSManagedLabelValueTrue,
				},
			},
		},
	}

	ctx := context.Background()

	// Create secret first time
	err := watcher.UpsertAviCredentialsSecret(ctx, cluster)
	if err != nil {
		t.Fatalf("First secret creation failed: %v", err)
	}

	// Get original secret directly from kube client
	secretName := "test-cluster-idempotency-avi-secret"
	originalSecret, err := setup.KubeClient.CoreV1().Secrets("test-namespace-idempotency").Get(ctx, secretName, metav1.GetOptions{})
	if err != nil {
		t.Fatalf("Failed to get original secret: %v", err)
	}

	// Verify the secret was created with expected content
	if originalSecret.Data == nil {
		t.Fatal("Secret data is nil")
	}

	expectedFields := []string{"username", "password", "controllerIP", "clusterName"}
	for _, field := range expectedFields {
		if _, exists := originalSecret.Data[field]; !exists {
			t.Errorf("Expected field %s not found in secret", field)
		}
	}

	// Manually add the created secret to the informer cache to simulate real behavior
	// This is necessary because fake clients don't automatically sync with informers
	err = utils.GetInformers().SecretInformer.Informer().GetStore().Add(originalSecret)
	if err != nil {
		t.Logf("Warning: Failed to add secret to informer cache: %v", err)
	}

	// Give time for the cache update to propagate
	time.Sleep(50 * time.Millisecond)

	// Try to handle the same provisioned cluster again (should be idempotent)
	err = watcher.HandleProvisionedCluster(cluster)
	if err != nil {
		t.Fatalf("Second HandleProvisionedCluster failed: %v", err)
	}

	// Verify secret still exists
	currentSecret, err := setup.KubeClient.CoreV1().Secrets("test-namespace-idempotency").Get(ctx, secretName, metav1.GetOptions{})
	if err != nil {
		t.Fatalf("Failed to get current secret: %v", err)
	}

	// Verify the secret still contains all expected data
	for _, field := range expectedFields {
		if _, exists := currentSecret.Data[field]; !exists {
			t.Errorf("Expected field %s not found in secret after second processing", field)
		}
	}

	// Verify that the secret content is still valid
	if len(currentSecret.Data) == 0 {
		t.Error("Secret data is empty after second processing")
	}

	t.Logf("Secret idempotency test passed - secret exists with %d fields", len(currentSecret.Data))
}

func TestVKSClusterWatcher_buildVKSClusterConfig(t *testing.T) {
	// Set up mock controller IP and version for consistent testing
	lib.SetControllerIP("10.10.10.10")
	originalVersion := lib.GetControllerVersion()
	defer func() {
		if originalVersion != "" {
			lib.AKOControlConfig().SetControllerVersion(originalVersion)
		}
	}()
	lib.AKOControlConfig().SetControllerVersion("22.1.3")

	// Set up mock T1LR path
	originalT1LR := os.Getenv("NSXT_T1_LR")
	defer func() {
		if originalT1LR != "" {
			os.Setenv("NSXT_T1_LR", originalT1LR)
		} else {
			os.Unsetenv("NSXT_T1_LR")
		}
	}()
	os.Setenv("NSXT_T1_LR", "/orgs/test-org/projects/test-project/vpcs/test-vpc")

	tests := []struct {
		name                 string
		clusterName          string
		clusterNamespace     string
		namespaceAnnotations map[string]string
		expectError          bool
		expectedSEG          string
		expectedTenant       string
		expectedT1LR         string
	}{
		{
			name:             "Successful config build with mock credentials",
			clusterName:      "test-cluster",
			clusterNamespace: "test-namespace",
			namespaceAnnotations: map[string]string{
				lib.WCPSEGroup:                 "custom-seg-group",
				lib.TenantAnnotation:           "custom-tenant",
				lib.InfraSettingNameAnnotation: "test-aviinfrasetting",
			},
			expectError:    false,
			expectedSEG:    "custom-seg-group",
			expectedTenant: "custom-tenant",
			expectedT1LR:   "/orgs/test-org/projects/test-project/vpcs/test-vpc",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			setup := setupVKSTest(t, tt.clusterName, tt.clusterNamespace, "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
			defer setup.Cleanup()

			// Update the test namespace with custom annotations
			// (since setupVKSTest already created the namespace with default annotations)
			namespace, err := setup.KubeClient.CoreV1().Namespaces().Get(context.Background(), tt.clusterNamespace, metav1.GetOptions{})
			if err != nil {
				t.Fatalf("Failed to get test namespace: %v", err)
			}
			namespace.Annotations = tt.namespaceAnnotations
			_, err = setup.KubeClient.CoreV1().Namespaces().Update(context.Background(), namespace, metav1.UpdateOptions{})
			if err != nil {
				t.Fatalf("Failed to update test namespace: %v", err)
			}

			// Admin secret no longer needed with cluster-specific RBAC

			watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

			// Set up mock credentials function
			watcher.SetTestMode(func(clusterName, operationalTenant string) (*lib.ClusterCredentials, error) {
				return &lib.ClusterCredentials{
					Username: fmt.Sprintf("vks-cluster-%s-user", clusterName),
					Password: "mock-password",
				}, nil
			})

			cluster := &unstructured.Unstructured{
				Object: map[string]interface{}{
					"metadata": map[string]interface{}{
						"name":      tt.clusterName,
						"namespace": tt.clusterNamespace,
						"uid":       "test-uid-123",
					},
				},
			}

			config, err := watcher.buildVKSClusterConfig(cluster)

			if tt.expectError {
				if err == nil {
					t.Error("Expected error but got none")
				}
			} else {
				if err != nil {
					t.Errorf("Expected no error but got: %v", err)
				}
				if config == nil {
					t.Fatal("Expected config but got nil")
				}

				// Validate VKS-specific fields
				if config.CNIPlugin != "antrea" {
					t.Errorf("Expected CNIPlugin to be 'antrea', got '%s'", config.CNIPlugin)
				}
				if config.ServiceType != "NodePortLocal" {
					t.Errorf("Expected ServiceType to be 'NodePortLocal', got '%s'", config.ServiceType)
				}

				// Note: In unit tests, we can't test the full RBAC creation
				// because it requires a real Avi Controller connection.
				// The main validation is that the function properly handles
				// the credential creation flow and namespace configuration.
			}
		})
	}
}

// Note: This test is removed because we no longer use local cache population.
// Credentials are now fetched directly from the secret informer cache on demand.

func TestVKSClusterWatcher_CNIServiceTypeMapping(t *testing.T) {
	// Test CNI detection and service type mapping integration

	// Test CNI detection and service type mapping
	tests := []struct {
		name                string
		cniRefName          string
		expectedCNI         string
		expectedServiceType string
	}{
		{
			name:                "Antrea CNI -> NodePortLocal",
			cniRefName:          "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1",
			expectedCNI:         "antrea",
			expectedServiceType: "NodePortLocal",
		},
		{
			name:                "Calico CNI -> NodePort",
			cniRefName:          "calico.tanzu.vmware.com.3.26.4+vmware.1-tkg.1",
			expectedCNI:         "calico",
			expectedServiceType: "NodePort",
		},
		{
			name:                "Cilium CNI -> NodePort",
			cniRefName:          "cilium.tanzu.vmware.com.1.14.5+vmware.1-tkg.1",
			expectedCNI:         "cilium",
			expectedServiceType: "NodePort",
		},
		{
			name:                "Flannel CNI -> NodePort",
			cniRefName:          "flannel.tanzu.vmware.com.0.22.2+vmware.1-tkg.1",
			expectedCNI:         "flannel",
			expectedServiceType: "NodePort",
		},
		{
			name:                "Unknown CNI -> NodePort",
			cniRefName:          "unknown-cni.tanzu.vmware.com.1.0.0+vmware.1-tkg.1",
			expectedCNI:         "",
			expectedServiceType: "NodePort",
		},
	}

	// Set up mock controller IP and T1LR
	lib.SetControllerIP("10.10.10.10")
	lib.AKOControlConfig().SetControllerVersion("22.1.3")

	// Set up mock T1LR path environment variable for tests
	originalT1LR := os.Getenv("NSXT_T1_LR")
	defer func() {
		if originalT1LR != "" {
			os.Setenv("NSXT_T1_LR", originalT1LR)
		} else {
			os.Unsetenv("NSXT_T1_LR")
		}
	}()
	os.Setenv("NSXT_T1_LR", "/orgs/test-org/projects/test-project/vpcs/test-vpc")

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			setup := setupVKSTest(t, "test-cluster", "test-namespace", tt.cniRefName)
			defer setup.Cleanup()

			// Admin secret no longer needed with cluster-specific RBAC

			watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

			// Set up mock credentials function
			watcher.SetTestMode(func(clusterName, operationalTenant string) (*lib.ClusterCredentials, error) {
				return &lib.ClusterCredentials{
					Username: fmt.Sprintf("vks-cluster-%s-user", clusterName),
					Password: "mock-password",
				}, nil
			})

			cluster := &unstructured.Unstructured{
				Object: map[string]interface{}{
					"metadata": map[string]interface{}{
						"name":      "test-cluster",
						"namespace": "test-namespace",
						"uid":       "test-uid-123",
					},
				},
			}

			config, err := watcher.buildVKSClusterConfig(cluster)
			if err != nil {
				t.Fatalf("Unexpected error: %v", err)
			}

			// Validate CNI detection
			if config.CNIPlugin != tt.expectedCNI {
				t.Errorf("Expected CNI '%s', got '%s'", tt.expectedCNI, config.CNIPlugin)
			}

			// Validate service type selection
			if config.ServiceType != tt.expectedServiceType {
				t.Errorf("Expected ServiceType '%s', got '%s'", tt.expectedServiceType, config.ServiceType)
			}
		})
	}
}

func TestVKSClusterWatcher_UpsertAviCredentialsSecret_Comprehensive(t *testing.T) {
	// Test comprehensive secret upsert operations with mocking

	// Set up mock controller IP
	lib.SetControllerIP("10.10.10.10")

	tests := []struct {
		name                 string
		clusterName          string
		clusterNamespace     string
		existingSecretData   map[string][]byte
		expectCreate         bool
		expectUpdate         bool
		expectError          bool
		expectedSecretFields []string
	}{
		{
			name:                 "Create new secret",
			clusterName:          "test-cluster-create",
			clusterNamespace:     "test-namespace-create",
			existingSecretData:   nil, // No existing secret
			expectCreate:         true,
			expectUpdate:         false,
			expectError:          false,
			expectedSecretFields: []string{"username", "password", "controllerIP", "clusterName"},
		},
		{
			name:             "Update existing secret with different data",
			clusterName:      "test-cluster-update",
			clusterNamespace: "test-namespace-update",
			existingSecretData: map[string][]byte{
				"username":                 []byte("old-admin"),
				"password":                 []byte("old-password"),
				"controllerIP":             []byte("10.10.10.9"), // Different IP
				"certificateAuthorityData": []byte("old-ca-cert"),
			},
			expectCreate:         false,
			expectUpdate:         true,
			expectError:          false,
			expectedSecretFields: []string{"username", "password", "controllerIP", "clusterName"},
		},
		{
			name:             "No update needed - identical secret",
			clusterName:      "test-cluster-nochange",
			clusterNamespace: "test-namespace-nochange",
			existingSecretData: map[string][]byte{
				"username":                 []byte("vks-cluster-test-cluster-nochange-user"),
				"password":                 []byte("mock-password"),
				"controllerIP":             []byte("10.10.10.10"),
				"certificateAuthorityData": []byte("mock-ca-cert"),
			},
			expectCreate:         false,
			expectUpdate:         false,
			expectError:          false,
			expectedSecretFields: []string{"username", "password", "controllerIP", "clusterName"},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			setup := setupVKSTest(t, tt.clusterName, tt.clusterNamespace, "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
			defer setup.Cleanup()

			// Initialize informers for the test (always initialize fresh informers for testing)
			registeredInformers := []string{
				utils.SecretInformer,
			}
			utils.NewInformers(utils.KubeClientIntf{ClientSet: setup.KubeClient}, registeredInformers, nil)

			// CRD informers already initialized by setupVKSTest

			// Set up CRD client for AviInfraSetting access (from integration test pattern)
			akoControlConfig := lib.AKOControlConfig()
			if akoControlConfig == nil {
				t.Fatalf("AKOControlConfig is nil")
			}

			// Create and set up the CRD client with a test AviInfraSetting
			testAviInfraSetting := createTypedAviInfraSetting("test-aviinfrasetting")
			v1beta1crdClient := v1beta1crdfake.NewSimpleClientset(testAviInfraSetting)
			akoControlConfig.SetCRDClientsetAndEnableInfraSettingParam(v1beta1crdClient)

			// Initialize CRD informers (this step is critical for getNamespaceConfig to work)
			k8s.NewCRDInformers()

			// Start the AviInfraSetting informer and wait for cache sync (like integration tests)
			stopCh := make(chan struct{})
			defer close(stopCh)
			go func() {
				defer close(stopCh)
				time.Sleep(30 * time.Second) // Allow test to complete
			}()
			go lib.RunAviInfraSettingInformer(stopCh)

			// Give the informer time to sync
			time.Sleep(100 * time.Millisecond)

			// Start informers once using sync.Once to avoid multiple starts
			startInformersOnce()

			// Create existing secret if provided
			secretName := fmt.Sprintf("%s-avi-secret", tt.clusterName)
			if tt.existingSecretData != nil {
				existingSecret := &corev1.Secret{
					ObjectMeta: metav1.ObjectMeta{
						Name:      secretName,
						Namespace: tt.clusterNamespace,
						Labels: map[string]string{
							"ako.kubernetes.vmware.com/cluster":    tt.clusterName,
							"ako.kubernetes.vmware.com/managed-by": "ako-infra",
						},
					},
					Type: corev1.SecretTypeOpaque,
					Data: tt.existingSecretData,
				}
				_, err := setup.KubeClient.CoreV1().Secrets(tt.clusterNamespace).Create(context.Background(), existingSecret, metav1.CreateOptions{})
				if err != nil {
					t.Fatalf("Failed to create existing secret: %v", err)
				}

				// Add existing secret to informer cache
				err = utils.GetInformers().SecretInformer.Informer().GetStore().Add(existingSecret)
				if err != nil {
					t.Logf("Warning: Failed to add existing secret to informer cache: %v", err)
				}
			}

			watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

			// Set up mock credentials function since this test calls buildVKSClusterConfig
			watcher.SetTestMode(func(clusterNameWithUID, operationalTenant string) (*lib.ClusterCredentials, error) {
				return &lib.ClusterCredentials{
					Username: fmt.Sprintf("vks-cluster-%s-user", clusterNameWithUID),
					Password: "mock-password",
				}, nil
			})

			cluster := &unstructured.Unstructured{
				Object: map[string]interface{}{
					"metadata": map[string]interface{}{
						"name":      tt.clusterName,
						"namespace": tt.clusterNamespace,
						"uid":       "test-uid-123",
					},
				},
			}

			ctx := context.Background()
			err := watcher.UpsertAviCredentialsSecret(ctx, cluster)

			if tt.expectError {
				if err == nil {
					t.Error("Expected error but got none")
				}
				return
			}

			if err != nil {
				t.Errorf("Expected no error but got: %v", err)
				return
			}

			// Verify secret exists after operation
			secret, err := setup.KubeClient.CoreV1().Secrets(tt.clusterNamespace).Get(ctx, secretName, metav1.GetOptions{})
			if err != nil {
				t.Fatalf("Expected secret to exist after upsert, got error: %v", err)
			}

			// Check labels
			expectedLabels := map[string]string{
				"ako.kubernetes.vmware.com/cluster":    tt.clusterName,
				"ako.kubernetes.vmware.com/managed-by": "ako-infra",
			}
			for key, expectedValue := range expectedLabels {
				if actualValue, exists := secret.Labels[key]; !exists || actualValue != expectedValue {
					t.Errorf("Expected label %s=%s, got %s=%s", key, expectedValue, key, actualValue)
				}
			}

			// Check expected fields are present
			for _, field := range tt.expectedSecretFields {
				if _, exists := secret.Data[field]; !exists {
					t.Errorf("Expected field %s to be present in secret data", field)
				}
			}

			// Verify that the operation completed successfully
			t.Logf("Test %s completed successfully: secret exists with %d fields", tt.name, len(secret.Data))

			// Verify that core fields are present (exact values may vary based on test scenario)
			coreFields := []string{"username", "controllerIP", "clusterName"}
			for _, field := range coreFields {
				if _, exists := secret.Data[field]; !exists {
					t.Errorf("Expected core field %s to be present in secret", field)
				}
			}
		})
	}
}

// TestVKSClusterWatcher_CleanupOnDeletion tests VKS cleanup when cluster is deleted
func TestVKSClusterWatcher_CleanupOnDeletion(t *testing.T) {
	// Set up mock controller IP
	lib.SetControllerIP("10.10.10.10")

	setup := setupVKSTest(t, "cleanup-test-cluster", "vks-cleanup-test-ns", "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
	defer setup.Cleanup()

	watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

	// Set up mock credentials function
	watcher.SetTestMode(func(clusterNameWithUID, operationalTenant string) (*lib.ClusterCredentials, error) {
		return &lib.ClusterCredentials{
			Username: fmt.Sprintf("vks-cluster-%s-user", clusterNameWithUID),
			Password: "mock-password",
		}, nil
	})

	// Create a VKS cluster with managed label
	cluster := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "cluster.x-k8s.io/v1beta2",
			"kind":       "Cluster",
			"metadata": map[string]interface{}{
				"name":      "cleanup-test-cluster",
				"namespace": "vks-cleanup-test-ns",
				"uid":       "test-uid-123",
				"labels": map[string]interface{}{
					webhook.VKSManagedLabel: webhook.VKSManagedLabelValueTrue,
				},
			},
			"status": map[string]interface{}{
				"phase": "Provisioned",
			},
		},
	}

	// Add cluster to fake client
	setup.DynamicClient.Tracker().Add(cluster)

	// Create cluster and its secret first
	ctx := context.Background()
	err := watcher.UpsertAviCredentialsSecret(ctx, cluster)
	if err != nil {
		t.Fatalf("Failed to create cluster secret: %v", err)
	}

	// Verify secret was created
	secretName := "cleanup-test-cluster-avi-secret"
	secret, err := setup.KubeClient.CoreV1().Secrets("vks-cleanup-test-ns").Get(ctx, secretName, metav1.GetOptions{})
	if err != nil {
		t.Fatalf("Expected secret to be created: %v", err)
	}
	if secret.Name != secretName {
		t.Errorf("Expected secret name %s, got %s", secretName, secret.Name)
	}

	// Now test cluster deletion (VKS always does comprehensive cleanup)
	err = watcher.cleanupClusterDependencies(ctx, "cleanup-test-cluster", "vks-cleanup-test-ns", "")
	if err != nil {
		t.Errorf("Cleanup should not fail: %v", err)
	}

	// Verify secret was cleaned up
	_, err = setup.KubeClient.CoreV1().Secrets("vks-cleanup-test-ns").Get(ctx, secretName, metav1.GetOptions{})
	if err == nil {
		t.Error("Expected secret to be deleted but it still exists")
	}

	t.Log("â VKS cluster cleanup on deletion test completed successfully")
}

// TestVKSClusterWatcher_CleanupOnOptOut tests VKS cleanup when cluster opts out
func TestVKSClusterWatcher_CleanupOnOptOut(t *testing.T) {
	// Set up mock controller IP
	lib.SetControllerIP("10.10.10.10")

	setup := setupVKSTest(t, "optout-test-cluster", "vks-optout-test-ns", "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
	defer setup.Cleanup()

	watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)

	// Set up mock credentials function
	watcher.SetTestMode(func(clusterNameWithUID, operationalTenant string) (*lib.ClusterCredentials, error) {
		return &lib.ClusterCredentials{
			Username: fmt.Sprintf("vks-cluster-%s-user", clusterNameWithUID),
			Password: "mock-password",
		}, nil
	})

	// Create a VKS cluster with managed label initially
	cluster := &unstructured.Unstructured{
		Object: map[string]interface{}{
			"apiVersion": "cluster.x-k8s.io/v1beta2",
			"kind":       "Cluster",
			"metadata": map[string]interface{}{
				"name":      "optout-test-cluster",
				"namespace": "vks-optout-test-ns",
				"uid":       "test-uid-456",
				"labels": map[string]interface{}{
					webhook.VKSManagedLabel: webhook.VKSManagedLabelValueTrue,
				},
			},
			"status": map[string]interface{}{
				"phase": "Provisioned",
			},
		},
	}

	// Add cluster to fake client
	setup.DynamicClient.Tracker().Add(cluster)

	// Create cluster and its secret first
	ctx := context.Background()
	err := watcher.UpsertAviCredentialsSecret(ctx, cluster)
	if err != nil {
		t.Fatalf("Failed to create cluster secret: %v", err)
	}

	// Verify secret was created
	secretName := "optout-test-cluster-avi-secret"
	secret, err := setup.KubeClient.CoreV1().Secrets("vks-optout-test-ns").Get(ctx, secretName, metav1.GetOptions{})
	if err != nil {
		t.Fatalf("Expected secret to be created: %v", err)
	}

	// Add the secret to the informer cache so HandleProvisionedCluster can find it
	err = utils.GetInformers().SecretInformer.Informer().GetStore().Add(secret)
	if err != nil {
		t.Logf("Warning: Failed to add secret to informer cache: %v", err)
	}

	// Now test cluster opt-out by changing the label
	cluster.SetLabels(map[string]string{
		webhook.VKSManagedLabel: webhook.VKSManagedLabelValueFalse,
	})

	// Update the cluster in the dynamic client to reflect the label change
	_, err = setup.DynamicClient.Resource(lib.ClusterGVR).Namespace("vks-optout-test-ns").Update(ctx, cluster, metav1.UpdateOptions{})
	if err != nil {
		t.Fatalf("Failed to update cluster for opt-out: %v", err)
	}

	// Process cluster opt-out via HandleProvisionedCluster
	err = watcher.HandleProvisionedCluster(cluster)
	if err != nil {
		t.Errorf("HandleProvisionedCluster should not fail on opt-out: %v", err)
	}

	// Verify secret was cleaned up (opt-out should trigger comprehensive cleanup)
	_, err = setup.KubeClient.CoreV1().Secrets("vks-optout-test-ns").Get(ctx, secretName, metav1.GetOptions{})
	if err == nil {
		t.Error("Expected secret to be deleted after opt-out but it still exists")
	}

	t.Log("â VKS cluster opt-out cleanup test completed successfully")
}

// TestVKSClusterWatcher_PeriodicReconciler tests the periodic reconciliation functionality
func TestVKSClusterWatcher_PeriodicReconciler(t *testing.T) {
	setup := setupVKSTest(t, "reconcile-test-cluster", "vks-reconcile-test-ns", "antrea.tanzu.vmware.com.2.3.0+vmware.1-tkg.1")
	defer setup.Cleanup()

	// Create multiple VKS clusters with different states
	clusters := []*unstructured.Unstructured{
		// Managed cluster in provisioned state - should be reconciled
		{
			Object: map[string]interface{}{
				"metadata": map[string]interface{}{
					"name":      "managed-cluster",
					"namespace": "vks-reconcile-test-ns",
					"uid":       "managed-uid-123",
					"labels": map[string]interface{}{
						webhook.VKSManagedLabel: webhook.VKSManagedLabelValueTrue,
					},
				},
				"status": map[string]interface{}{
					"phase": ClusterPhaseProvisioned,
				},
			},
		},
		// Unmanaged cluster - should be skipped
		{
			Object: map[string]interface{}{
				"metadata": map[string]interface{}{
					"name":      "unmanaged-cluster",
					"namespace": "vks-reconcile-test-ns",
					"uid":       "unmanaged-uid-456",
					"labels": map[string]interface{}{
						webhook.VKSManagedLabel: webhook.VKSManagedLabelValueFalse,
					},
				},
				"status": map[string]interface{}{
					"phase": ClusterPhaseProvisioned,
				},
			},
		},
		// Managed cluster in deleting state - should be skipped
		{
			Object: map[string]interface{}{
				"metadata": map[string]interface{}{
					"name":      "deleting-cluster",
					"namespace": "vks-reconcile-test-ns",
					"uid":       "deleting-uid-789",
					"labels": map[string]interface{}{
						webhook.VKSManagedLabel: webhook.VKSManagedLabelValueTrue,
					},
				},
				"status": map[string]interface{}{
					"phase": ClusterPhaseDeleting,
				},
			},
		},
	}

	// Add clusters to dynamic client
	for _, cluster := range clusters {
		_, err := setup.DynamicClient.Resource(lib.ClusterGVR).Namespace("vks-reconcile-test-ns").Create(context.Background(), cluster, metav1.CreateOptions{})
		if err != nil {
			t.Fatalf("Failed to create cluster %s: %v", cluster.GetName(), err)
		}
	}

	watcher := NewVKSClusterWatcher(setup.KubeClient, setup.DynamicClient)
	watcher.SetTestMode(func(clusterNameWithUID, operationalTenant string) (*lib.ClusterCredentials, error) {
		return &lib.ClusterCredentials{
			Username: fmt.Sprintf("vks-cluster-%s-user", clusterNameWithUID),
			Password: "mock-password",
		}, nil
	})

	// Test reconcileAllClusters method directly
	watcher.reconcileAllClusters()

	// Give some time for items to be processed
	time.Sleep(100 * time.Millisecond)

	// We'll just verify the method doesn't panic and logs appropriately
	// The reconciler should have processed 1 managed cluster and skipped 2 others

	t.Log("â VKS periodic reconciler test completed successfully")
}
